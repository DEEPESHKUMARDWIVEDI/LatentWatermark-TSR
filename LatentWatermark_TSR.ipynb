{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f91445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.5868 | Test Acc: 88.41%\n",
      "✅ Saved best model checkpoint with accuracy 88.41%\n",
      "Epoch [2/10] - Loss: 0.0727 | Test Acc: 91.85%\n",
      "✅ Saved best model checkpoint with accuracy 91.85%\n",
      "Epoch [3/10] - Loss: 0.0365 | Test Acc: 93.18%\n",
      "✅ Saved best model checkpoint with accuracy 93.18%\n",
      "Epoch [4/10] - Loss: 0.0216 | Test Acc: 92.15%\n",
      "Epoch [5/10] - Loss: 0.0213 | Test Acc: 93.19%\n",
      "✅ Saved best model checkpoint with accuracy 93.19%\n",
      "Epoch [6/10] - Loss: 0.0221 | Test Acc: 93.88%\n",
      "✅ Saved best model checkpoint with accuracy 93.88%\n",
      "Epoch [7/10] - Loss: 0.0153 | Test Acc: 93.90%\n",
      "✅ Saved best model checkpoint with accuracy 93.90%\n",
      "Epoch [8/10] - Loss: 0.0121 | Test Acc: 92.65%\n",
      "Epoch [9/10] - Loss: 0.0175 | Test Acc: 93.63%\n",
      "Epoch [10/10] - Loss: 0.0090 | Test Acc: 93.98%\n",
      "✅ Saved best model checkpoint with accuracy 93.98%\n",
      "Training completed. Best accuracy: 93.98%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.gtsrb_dataset import GTSRBDataset\n",
    "from models.tsr_cnn import TSRNet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset setup\n",
    "# ------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = GTSRBDataset(csv_file=\"./data/Train.csv\", root_dir=\"./data/\", transform=transform)\n",
    "test_dataset  = GTSRBDataset(csv_file=\"./data/Test.csv\",  root_dir=\"./data/\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ------------------------------\n",
    "# Model, Loss, Optimizer\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TSRNet(num_classes=43).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------------\n",
    "# Checkpoint setup\n",
    "# ------------------------------\n",
    "ckpt_dir = \"./results/checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_acc = 0.0\n",
    "\n",
    "# ------------------------------\n",
    "# Training loop\n",
    "# ------------------------------\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # --------------------------\n",
    "    # Evaluation after each epoch\n",
    "    # --------------------------\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss/len(train_loader):.4f} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Save best checkpoint\n",
    "    # --------------------------\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        ckpt_path = os.path.join(ckpt_dir, f\"tsr_best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'accuracy': best_acc,\n",
    "        }, ckpt_path)\n",
    "        print(f\"✅ Saved best model checkpoint with accuracy {best_acc:.2f}%\")\n",
    "\n",
    "print(f\"Training completed. Best accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded checkpoint: epoch 10, acc 93.98258115597783\n",
      "Clean Test Accuracy: 93.98%\n",
      "\n",
      "FGSM Results (eps=0.03):\n",
      "  Clean Accuracy: 93.98%\n",
      "  Adversarial Accuracy: 52.64%\n",
      "  Attack Success Rate (ASR): 43.99%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m grid_pert \u001b[38;5;241m=\u001b[39m make_grid(pert, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    118\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 119\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[43mgrid_orig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    120\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# FGSM / PGD Attack Evaluation with ASR\n",
    "# ==================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from utils.attacks import fgsm_attack, pgd_attack\n",
    "from utils.visualization import save_sample\n",
    "from models.tsr_cnn import TSRNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# Load trained TSR model\n",
    "# -----------------------------\n",
    "ckpt_path = \"./results/checkpoints/tsr_best_model.pth\"\n",
    "model = TSRNet(num_classes=43).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    try:\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"Loaded checkpoint: epoch {ckpt.get('epoch', '?')}, acc {ckpt.get('accuracy', '?')}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Checkpoint not found at \" + ckpt_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    acc = 100.0 * correct / total\n",
    "    return acc, torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on Clean Data\n",
    "# -----------------------------\n",
    "clean_acc, clean_preds, clean_labels = evaluate(model, test_loader, device)\n",
    "print(f\"Clean Test Accuracy: {clean_acc:.2f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# FGSM Attack\n",
    "# -----------------------------\n",
    "eps = 0.03  # attack strength\n",
    "model.eval()\n",
    "adv_correct, adv_total = 0, 0\n",
    "correct_to_wrong = 0\n",
    "original_correct = 0\n",
    "\n",
    "for imgs, labels in test_loader:\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    outputs = model(imgs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    # Count correctly classified samples\n",
    "    correct_mask = (preds == labels)\n",
    "    original_correct += correct_mask.sum().item()\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    adv_imgs = fgsm_attack(model, imgs, labels, eps=eps, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        adv_outputs = model(adv_imgs)\n",
    "        _, adv_preds = torch.max(adv_outputs, 1)\n",
    "    \n",
    "    adv_correct += (adv_preds == labels).sum().item()\n",
    "    adv_total += labels.size(0)\n",
    "    \n",
    "    # Count those flipped from correct → wrong\n",
    "    correct_to_wrong += ((preds == labels) & (adv_preds != labels)).sum().item()\n",
    "\n",
    "adv_acc = 100.0 * adv_correct / adv_total\n",
    "attack_success_rate = 100.0 * correct_to_wrong / max(original_correct, 1)\n",
    "\n",
    "print(f\"\\nFGSM Results (eps={eps}):\")\n",
    "print(f\"  Clean Accuracy: {clean_acc:.2f}%\")\n",
    "print(f\"  Adversarial Accuracy: {adv_acc:.2f}%\")\n",
    "print(f\"  Attack Success Rate (ASR): {attack_success_rate:.2f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization (Fixed for CUDA)\n",
    "# -----------------------------\n",
    "imgs_batch, labels_batch = next(iter(test_loader))\n",
    "imgs_batch, labels_batch = imgs_batch.to(device), labels_batch.to(device)\n",
    "adv_imgs_batch = fgsm_attack(model, imgs_batch, labels_batch, eps=eps, device=device)\n",
    "\n",
    "def denorm(x):\n",
    "    return torch.clamp(x * 0.5 + 0.5, 0, 1)  # inverse of normalization\n",
    "\n",
    "orig = denorm(imgs_batch[:8])\n",
    "adv = denorm(adv_imgs_batch[:8])\n",
    "pert = (adv - orig).abs() * 10.0  # amplify perturbation\n",
    "\n",
    "grid_orig = make_grid(orig.cpu(), nrow=4)\n",
    "grid_adv = make_grid(adv.cpu(), nrow=4)\n",
    "grid_pert = make_grid(pert.cpu(), nrow=4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(np.transpose(grid_orig.numpy(), (1,2,0)))\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(np.transpose(grid_adv.numpy(), (1,2,0)))\n",
    "axes[1].set_title(f\"Adversarial (ε={eps})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(np.transpose(grid_pert.numpy(), (1,2,0)))\n",
    "axes[2].set_title(\"Perturbation ×10\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "os.makedirs(\"results/sample_outputs\", exist_ok=True)\n",
    "save_sample(orig, \"results/sample_outputs/original_grid.png\")\n",
    "save_sample(adv, \"results/sample_outputs/adversarial_grid.png\")\n",
    "save_sample(pert, \"results/sample_outputs/perturbation_grid.png\")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional PGD Attack\n",
    "# -----------------------------\n",
    "run_pgd = False  # toggle True to test PGD\n",
    "if run_pgd:\n",
    "    eps, alpha, iters = 0.03, 0.007, 10\n",
    "    print(f\"\\nRunning PGD (eps={eps}, alpha={alpha}, iters={iters})...\")\n",
    "    pgd_correct, pgd_total, correct_to_wrong = 0, 0, 0\n",
    "    original_correct = 0\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_mask = (preds == labels)\n",
    "        original_correct += correct_mask.sum().item()\n",
    "        \n",
    "        adv_imgs = pgd_attack(model, imgs, labels, eps=eps, alpha=alpha, iters=iters, device=device)\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_imgs)\n",
    "            _, adv_preds = torch.max(adv_outputs, 1)\n",
    "        pgd_correct += (adv_preds == labels).sum().item()\n",
    "        pgd_total += labels.size(0)\n",
    "        correct_to_wrong += ((preds == labels) & (adv_preds != labels)).sum().item()\n",
    "    \n",
    "    pgd_acc = 100.0 * pgd_correct / pgd_total\n",
    "    pgd_asr = 100.0 * correct_to_wrong / max(original_correct, 1)\n",
    "    print(f\"PGD Accuracy: {pgd_acc:.2f}%  |  Attack Success Rate: {pgd_asr:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latenttsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
